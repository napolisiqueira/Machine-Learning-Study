{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f47095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time \n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a66681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor() #definindo a conversão de imagem para tensor\n",
    "\n",
    "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform) # Carrega a parte de treino do Dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes\n",
    "\n",
    "valset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform) # Carrega parte para validação do Dataset\n",
    "valloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True) # Cria um buffer para pegar os dados por partes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4ee735c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagens, etiquetas = next(iter(trainloader))\n",
    "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecbe1922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(imagens[0].shape) # Verifica a dimensão do tensor de cada imagem\n",
    "print(etiquetas[0].shape) # Verifica a dimensão do tensor de cada etiqueta\n",
    "\n",
    "torch.Size([1, 28, 28])\n",
    "torch.Size([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdebedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modelo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modelo, self).__init__()\n",
    "        self.linear1 = nn.Linear(28*28, 128) # camada de entrada, 784 neuronios que se ligam a 128\n",
    "        self.linear2 = nn.Linear(128, 64) # camada interna 1, 128 neuronios que se ligam a 64\n",
    "        self.linear3 = nn.Linear(64, 10) # camada interna 2, 64 neuronios que se ligam a 10\n",
    "        # paar a camada de sida não é necessario definir nada pois só precisamnos pegar o output da camada interna 2\n",
    "\n",
    "    def forwerd(self, X):\n",
    "        X = F.relu(self.linear1(X)) # função de ativação da camada de entrada para camada interna 1\n",
    "        X = F.relu(self.linear2(X)) # função de ativação da camada interna 1 para a camada interna 2\n",
    "        X = F.relu(self.linear(X)) # função de ativação da camada interna 2 para a camada de saída, nesse caso f(x) = x\n",
    "        return F.log_softmax(X, dim=1) # dados utilizados para calcular perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino(modelo, trainloader, device):\n",
    "    \n",
    "    otimizador = optim.SGD(modelo.parameters(), lr=0.001, momentum=0.5) # Definindo a politica de atualização dos pesos e da bias\n",
    "    inicio = time() # Iniciando a contagem do tempo de treino\n",
    "    \n",
    "    criterio = nn.NLLLoss() # Definindo o critério para calcular a perda\n",
    "    EPOCHS = 10 # Definindo o número de épocas que o modelo irá treinar\n",
    "    modelo.train() # Definindo o modelo para treino\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        perda_acumulada = 0 # Definindo a perda acumulada para cada época\n",
    "        \n",
    "        for imagens, etiquetas in trainloader: # Iterando sobre o dataset de treino\n",
    "            \n",
    "            imagens = imagens.view(imagens.shape[0], -1) # Redimensionando as imagens para o formato de vetor de 28x28 para ficarem compatíveis com a camada de entrada\n",
    "            otimizador.zero_grad() # Zerando os gradientes por conta do cliclo anterior\n",
    "            \n",
    "            output = modelo(imagens.to(device)) # Colocando os dados no modelo\n",
    "            perda_instantanea = criterio(output, etiquetas.to(device)) # Calculando a perda da epoch em questão\n",
    "            \n",
    "            perda_instantanea.backward() # Calculando os gradientes\n",
    "            \n",
    "            otimizador.step() # Atualizando os pesos e bias\n",
    "            \n",
    "            perda_acumulada += perda_instantanea.item() # Acumulando a perda da epoch em questão\n",
    "            \n",
    "        else:\n",
    "            print(\"Epoch {} - Perda resultante: {}\".format(epoch+1, perda_acumulada/len(trainloader))) # Imprimindo a perda da epoch em questão\n",
    "    \n",
    "    print(\"\\nTempo de treino (em minutos) = \", (time()-inicio)/60) # Imprimindo o tempo de treino em minutos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86014f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao(modelo, valloader, device):\n",
    "    conta_corretas, conta_todas = 0, 0 # Definindo as variáveis para contar as predições corretas e todas as predições\n",
    "    for imagens, etiquetas in valloader:\n",
    "        for i in range(len(etiquetas)):\n",
    "            img = imagens[i].view(1, 784) # Redimensionando a imagem para o formato de vetor de 28x28 para ficarem compatíveis com a camada de entrada\n",
    "            # Desativar o autograd para acelerar o processo de validação. Grafos computacionais dinâmicos tem um curto alto de processamento\n",
    "            with torch.no_grad():\n",
    "                logps = modelo(img.to(device)) # Colocando os dados no modelo\n",
    "                \n",
    "            ps = torch.exp(logps) # Calculando a probabilidade de cada classe\n",
    "            probab = list(ps.cpu().numpy()[0]) # Convertendo para numpy e pegando a probabilidade de cada classe\n",
    "            etiqueta_pred = probab.index(max(probab)) # Pegando a classe com maior probabilidade\n",
    "            etiqueta_certa = etiquetas.numpy()[i] # Pegando a classe correta\n",
    "            \n",
    "            if(etiqueta_certa == etiqueta_pred): # Verificando se a predição foi correta\n",
    "                conta_corretas += 1\n",
    "            conta_todas += 1\n",
    "            \n",
    "    print(\"Total de imagens: \", conta_todas) # Imprimindo o total de imagens\n",
    "    print(\"\\nPrecisão do modelo = {:.2f}%\".format((conta_corretas/conta_todas)*100)) # Imprimindo a precisão do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6836574",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Modelo()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Definindo o dispositivo de processamento\n",
    "modelo.to(device) # Colocando o modelo no dispositivo de processamento"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-Learning-Study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
